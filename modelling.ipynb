{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "from keras import layers, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data from the pickle file in the data/ directory\n",
    "df = pd.read_pickle('data/words_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WordID</th>\n",
       "      <th>SegmentationResult</th>\n",
       "      <th>GrayLevel</th>\n",
       "      <th>BoundingBox</th>\n",
       "      <th>GrammaticalTag</th>\n",
       "      <th>Transcription</th>\n",
       "      <th>ImageData</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a01-000u-00-00</td>\n",
       "      <td>ok</td>\n",
       "      <td>154</td>\n",
       "      <td>(408, 768, 27, 51)</td>\n",
       "      <td>AT</td>\n",
       "      <td>A</td>\n",
       "      <td>[[[0.9764706], [0.9764706], [0.9764706], [0.97...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a01-000u-00-01</td>\n",
       "      <td>ok</td>\n",
       "      <td>154</td>\n",
       "      <td>(507, 766, 213, 48)</td>\n",
       "      <td>NN</td>\n",
       "      <td>MOVE</td>\n",
       "      <td>[[[1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a01-000u-00-02</td>\n",
       "      <td>ok</td>\n",
       "      <td>154</td>\n",
       "      <td>(796, 764, 70, 50)</td>\n",
       "      <td>TO</td>\n",
       "      <td>to</td>\n",
       "      <td>[[[0.9843137], [0.9843137], [0.9843137], [0.98...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a01-000u-00-03</td>\n",
       "      <td>ok</td>\n",
       "      <td>154</td>\n",
       "      <td>(919, 757, 166, 78)</td>\n",
       "      <td>VB</td>\n",
       "      <td>stop</td>\n",
       "      <td>[[[1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a01-000u-00-04</td>\n",
       "      <td>ok</td>\n",
       "      <td>154</td>\n",
       "      <td>(1185, 754, 126, 61)</td>\n",
       "      <td>NPT</td>\n",
       "      <td>Mr.</td>\n",
       "      <td>[[[0.9647059], [0.90588236], [0.80784315], [0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           WordID SegmentationResult  GrayLevel           BoundingBox  \\\n",
       "0  a01-000u-00-00                 ok        154    (408, 768, 27, 51)   \n",
       "1  a01-000u-00-01                 ok        154   (507, 766, 213, 48)   \n",
       "2  a01-000u-00-02                 ok        154    (796, 764, 70, 50)   \n",
       "3  a01-000u-00-03                 ok        154   (919, 757, 166, 78)   \n",
       "4  a01-000u-00-04                 ok        154  (1185, 754, 126, 61)   \n",
       "\n",
       "  GrammaticalTag Transcription  \\\n",
       "0             AT             A   \n",
       "1             NN          MOVE   \n",
       "2             TO            to   \n",
       "3             VB          stop   \n",
       "4            NPT           Mr.   \n",
       "\n",
       "                                           ImageData  \n",
       "0  [[[0.9764706], [0.9764706], [0.9764706], [0.97...  \n",
       "1  [[[1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1...  \n",
       "2  [[[0.9843137], [0.9843137], [0.9843137], [0.98...  \n",
       "3  [[[1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1...  \n",
       "4  [[[0.9647059], [0.90588236], [0.80784315], [0....  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting train, validation, and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a01-117-05-02 will be deleted\n",
      "r06-022-03-05 will be deleted\n",
      "115318\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "c2=0\n",
    "for i, img in enumerate(df['ImageData']):\n",
    "    try:\n",
    "        if img.shape == (32, 128, 1):  # Replace with your expected dimensions\n",
    "            count+=1\n",
    "    except AttributeError:\n",
    "        print(f\"{df['WordID'][i]} will be deleted\")\n",
    "        df.drop([i], inplace=True)\n",
    "        c2+=1\n",
    "print(count)\n",
    "print(c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 images data weren't loaded into the dataframe. Delete those rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13550\n"
     ]
    }
   ],
   "source": [
    "print(len(df['Transcription'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n"
     ]
    }
   ],
   "source": [
    "# calculate the number of unique letters used in the values of the Transcription column\n",
    "unique_letters = set()\n",
    "for word in df['Transcription']:\n",
    "    unique_letters.update(word)\n",
    "print(len(unique_letters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n"
     ]
    }
   ],
   "source": [
    "# calculate the longest word\n",
    "max_len = 0\n",
    "for word in df['Transcription']:\n",
    "    if len(word) > max_len:\n",
    "        max_len = len(word)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "X = np.stack(df['ImageData'].values)  # Convert the list of images to a numpy array\n",
    "y = df['Transcription'].values\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create a label encoder for the transcriptions\n",
    "encoder = LabelEncoder()\n",
    "y_encoded = encoder.fit_transform(y)\n",
    "\n",
    "# One-hot encode the labels\n",
    "vocab_size = len(encoder.classes_)\n",
    "y_encoded = np.eye(vocab_size)[y_encoded].astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Starting simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple neural network model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Flatten(input_shape=(32, 128, 1)))  # Flatten the input\n",
    "model.add(layers.Dense(128, activation='relu'))  # A dense layer with ReLU activation\n",
    "model.add(layers.Dense(10, activation='softmax'))  # Output layer with softmax activation; change '10' to your number of classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
